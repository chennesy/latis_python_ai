{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863162ad-6e1c-4fec-894c-5c4fbec036e3",
   "metadata": {},
   "source": [
    "# Text analysis with AI\n",
    "A workshop by UMN LATIS and the Libraries.\n",
    "\n",
    "## What we'll cover in this session\n",
    "- Understand text classification with LLMs, and how it can be useful for text-based research.\n",
    "- Interact with the ChatGPT API.\n",
    "- Structure API calls using different models and prompts\n",
    "- Set up classification tasks with ChatGPT.\n",
    "- Understand and parse API JSON responses.\n",
    "- Understand risks in using generative AI for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496bad16-9c12-47d1-90b8-26f29baecbc0",
   "metadata": {},
   "source": [
    "### Install libraries\n",
    "If you're working from your own machine you can use pip install to make sure you have downloaded all of the Python packages you'll need to use today. \n",
    "\n",
    "If you're working on notebooks.latis.umn.edu, there's no need to install any of these, since they're included in the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac31f92-047c-49bc-b862-7d57dca9e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install Libraries ###\n",
    "\n",
    "#!pip install --upgrade openai python-dotenv\n",
    "#!pip install spacy\n",
    "#!pip install spacy-llm\n",
    "\n",
    "# This command downloads the medium-sized English language model for spaCy.\n",
    "# It uses the Python module-running option to run spaCy's download command for the \"en_core_web_md\" model.\n",
    "#!python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda311ed-7c9c-4c5a-be61-bd0c7ed10a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be59a016-04f2-4beb-bb54-0b9dca5e0d06",
   "metadata": {},
   "source": [
    "# ChatGPT: website vs. API\n",
    "\n",
    "The [chat interface on the website](https://chatgpt.com) is the most familiar way of interacting with these models.\n",
    "\n",
    "But we will be working with the [application programming interface (API)](https://en.wikipedia.org/wiki/API) to automatically send and receive messages from the model using some features that are not accessible via the web.\n",
    "\n",
    "## OpenAI's API\n",
    "\n",
    "Many applications and websites offer APIs. For example, nearly every weather app uses [the National Weather Service API](https://www.weather.gov/documentation/services-web-api) to automatically retrieve weather data.\n",
    "\n",
    "Unlike the National Weather Service, OpenAI charges for the use of its API. Which means that calls to the API require a special string called a `key`.\n",
    "\n",
    "### Getting a key\n",
    "\n",
    "After installing the Python bindings above (`openai`), you need to get an API key to send requests. The key is a unique identifier that performs a number of functions (including allowing OpenAI to bill you).\n",
    "\n",
    "For the purposes of this class, I have created a fresh key with a spending limit of `$10` that I will share with the group, which should be more than enough to satisfy all of the requests in this class.\n",
    "\n",
    "When you want to run your own queries in the future, you will need to register for an account and create an API key.\n",
    "\n",
    "See [this page of the documentation](https://platform.openai.com/docs/quickstart) for details of how to create your own key.\n",
    "\n",
    "### Setting the key\n",
    "\n",
    "You need to include the key with every call to the API.\n",
    "\n",
    "One way to do this is by setting the `OPENAI_API_KEY=...` variable in a `.env` file in your working directory.\n",
    "\n",
    "You can also do this by setting a local variable, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d82ea-a822-46e4-9522-db60171ab8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"  # copy-paste the class key here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7636e15-9714-445c-972f-8c0f322113c0",
   "metadata": {},
   "source": [
    "We're also going to write this to your `.env` so you don't have to repeat the process next time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc80b4-87c2-4f23-9a23-f5a3f94a0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(f\"OPENAI_API_KEY={OPENAI_API_KEY}\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f37ec4-1633-413b-9f82-1a02427d78c2",
   "metadata": {},
   "source": [
    "Next time your restart this notebook kernel (or open up a new notebook), the `openai` library will read the API key directly from your `.env` file. No need to specify the `api_key=` argument in `OpenAI()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0293b-eeea-4d23-9abc-43e1c4f93196",
   "metadata": {},
   "source": [
    "## Making your first API call\n",
    "\n",
    "You installed and imported the `openai` library above, so now you can run the example completion below, which is part of [OpenAI's tutorial](https://platform.openai.com/docs/api-reference/chat/create):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e59d79-ed15-41c9-83bb-5554cd6074cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will load your saved .env variable\n",
    "load_dotenv()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0b0d8-3a66-4329-9613-324ea7d37210",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert in qualitative data analysis. Your task is to analyze open-ended survey responses and categorize them into positive, negative, or neutral sentiments.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Survey Response: I feel overwhelmed by the workload but am excited about the learning opportunities.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab471885-c5c4-4c1d-a362-6d5824c49f65",
   "metadata": {},
   "source": [
    "Let's breakdown the elements from the code above:\n",
    "- `client.chat.completions.create()` calls the REST API chat completions endpoint\n",
    "- `model=\"gpt-4o-mini\"` - you can choose from a variety of [ChatGPT models](https://platform.openai.com/docs/models). We're using a lightweight (affordable) `gpt-40-mini` model. To get slightly more intelligent responses you could switch to `gpt-40`.\n",
    "- `messages` is a list of dictionaries contains messages sent to the `model`.\n",
    "  - There are two different values given for `role` in this example: `system` and `user`.\n",
    "  - `system` refers to the system message given to the LLM that conditions its reponses.\n",
    "\n",
    "Note how the output below differs from the output above, only changing the `system` message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464742c-b0a4-4a11-8102-21022b93d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an HR employee. Your task is to analyze open-ended survey responses and assess how employee's are feeling.\"\n",
    "user_message = \"Survey Response: I feel overwhelmed by the workload but am excited about the learning opportunities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6270e5-b7f8-46e1-a0d8-f97ae4e64744",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_message,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51902a0-e73a-4266-bb41-07b72043a7dc",
   "metadata": {},
   "source": [
    "You can explore the response by hitting tab after `completion.`. \n",
    "- `completion.usage` shows you how many tokens you sent and received. The number of tokens for each model corresponds to what you will be charged for your API usage. See [OpenAI's pricing page](https://openai.com/api/pricing/) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7176d-b680-4a67-8c89-baad6577e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921bf863-2b02-456b-af73-639de62cd570",
   "metadata": {},
   "source": [
    "- `completion.choices[0].` refers to the first response from the API. In our case we only have one response, but other queries can return a list of different API responses at different indices (e.g., `completion.choices[1]`).\n",
    "- `completion.choices[0].message.content` has the response content that we're interested in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e3aeea-45a7-4dd2-9b24-bc589e595ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f8ae0-d471-4b51-b3e7-0fa54d82edca",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Create a new system message and user message to send to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2aca7-fbcc-426c-9162-2a02c6fec730",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\n",
    "user_message = \"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_completion_tokens = 150, # this sets a maximum length of the response to keep our API costs down\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_message,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720212b6-493c-492f-b381-f364e0cbecae",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4a37c-7e43-415f-9ea1-69dcc72d1f33",
   "metadata": {},
   "source": [
    "Let's see how we can adjust the creativity and randomness of ChatGPT's response using the `temperature` and `top_p` parameters. \n",
    "- Temperature: We can turn down temperature to be get more determinative responses, or turn up the temperature to be more creative.\n",
    "- Top-p: Also known as \"nucleus sampling,\" `top_p` reduces the number of probable next words that will be considered in the response. Lower top_p figures reduce creativity, while a top_p of 1 leaves in 100% of probable next words (the default).\n",
    "\n",
    "We can copy and paste the code above as a starting place, and then add a new system and user prompt, along with our temperature parameter.\n",
    "\n",
    "See how the response changes when you adjust the `temperature` and `top_p` values between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5223aa-88e3-4601-b5a9-0940c2f490ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Provide one short innovative idea to address survey responses about public needs.\"\n",
    "user_message = \"Survey Response: I worry about global warming.\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    max_completion_tokens = 150, # this sets a maximum length of the response to keep our API costs down\n",
    "    temperature=.9,\n",
    "    top_p = .9,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_message,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08934891-72ff-4d35-8790-3c58ddf8c09b",
   "metadata": {},
   "source": [
    "### Using LLMs for classification \n",
    "\n",
    "We can write a simple system prompt to ask to classify text into various categories. First let's create a function for our API call to make it easier to reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029da1a7-d583-434c-a844-dd8c8b4736a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_call(system_message, user_message):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        max_completion_tokens = 100,\n",
    "        temperature=.2,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_message,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    print(f\"Headline: {user_message} \\nClassification: {response}\")\n",
    "    print()\n",
    "    return user_message, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad827bc-f956-4130-ab0a-32f259e3fef8",
   "metadata": {},
   "source": [
    "We can use a docstring - three double quotes - to delineate multi-line strings in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b4fd8-f2b3-418b-86c5-4d6533db54ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"Classes: \n",
    "['U.S.', 'World', 'Business', 'Arts', 'Lifestyle', 'Opinion', 'Sports', 'Science', 'Other']\n",
    "Classify the user input (newspaper headlines) into one of the above classes. \n",
    "If the headlines doesn't match a category, respond 'Other'.\"\"\"\n",
    "\n",
    "user_message = \"After a decade, scientists unveil fly brain in stunning detail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80756b9a-5af7-46c2-aaa0-ccbd7b6bb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message, response = api_call(system_message, user_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d4782-a29a-42b6-9adb-a0d523ecdfac",
   "metadata": {},
   "source": [
    "#### Newspaper headlines\n",
    "Let's import a list of newspaper headlines from the US, [collected on Kaggle](https://www.kaggle.com/datasets/felixludos/babel-briefings).\n",
    "\n",
    "The dataset is in JSON format, so we'll import the JSON library to work with the data and load it in a similar way as text files. `json.load` converts the JSON data into a Python object we can work with as a list of dictionaries for each headline \"item\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22900130-d61e-45ef-8f3d-1fb5e330c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# US headlines from https://www.kaggle.com/datasets/felixludos/babel-briefings?resource=download\n",
    "\n",
    "with open('data/babel-briefings-v1-us.json') as json_data:\n",
    "    headlines = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57333d7-b545-453e-84f8-2de22eb0f37f",
   "metadata": {},
   "source": [
    "Let's take a look at the dictionary for a single item in the headlines list. We want to work with the 'title' for each item, which is accessible via the dictionary key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4ac8c-53e2-4094-bcc3-1b44981b5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f554b-be13-4910-a942-26effee9b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e944d55-faac-4aa9-b997-4e1eec489fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's print out classifications for the first ten items\n",
    "for headline in headlines[0:10]:\n",
    "    api_call(system_message, headline['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1071eb-6998-42fa-9adb-6262f861cc30",
   "metadata": {},
   "source": [
    "### Using LLMs for Named Entity Recognition (NER)\n",
    "We can use the same technique, with a different system prompt, to ask for named entities (people, places, and other formal nouns) from each headline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3aa787-b4e5-482f-a296-dea3f95c63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"For each user input (newspaper headlines), give me a list of:\n",
    "- organization named entity\n",
    "- location named entity\n",
    "- person named entity\n",
    "Format the output in valid json with the following keys:\n",
    "- Organizations\n",
    "- Locations\n",
    "- Persons\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c0837c-843f-4e63-9581-4a7ad251798e",
   "metadata": {},
   "source": [
    "Instead of just printing our results, let's save them to a new Python dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4290b6-81c2-448a-8bf0-7cf74dfd2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_ner = {}\n",
    "for headline in headlines[20:30]:\n",
    "    headline, response = api_call(system_message, headline['title'])\n",
    "    headline_ner[headline] = response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181a76e-511e-4f7d-8952-ec09d0756569",
   "metadata": {},
   "source": [
    "Some of the JSON is not valid, despite our prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c327342-ea97-42b4-a4e8-471d7afcd46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in headline_ner.items():\n",
    "    print(k)\n",
    "    print(json.loads(v))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac69f0-7c05-4899-8ec3-77e531ee3aee",
   "metadata": {},
   "source": [
    "### Get better JSON\n",
    "\n",
    "There are ways to force ChatGPT to structure outputs using specific formats, as detailed in this post about the [`response_format` parameter](https://openai.com/index/introducing-structured-outputs-in-the-api/).\n",
    "\n",
    "Let's create a new API call focused on providing JSON responses for NER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9eb110-ca7b-48d9-90ed-a0cdb5e7b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_api_call(user_message):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        max_completion_tokens = 100,\n",
    "        temperature=.2,\n",
    "        response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"ner_schema\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"Organizations\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                        \"Locations\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                        \"Persons\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                },\n",
    "                \"required\": [\"Organizations\", \"Locations\", \"Persons\"],\n",
    "                \"additionalProperties\": False\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"For each user input (newspaper headlines), give me a JSON list of named entities including locations, organizations, and persons.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_message,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    print(f\"Headline: {user_message} \\nClassification: {response}\")\n",
    "    print()\n",
    "    return user_message, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcceeb-a560-4c5f-974f-1eefc286d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_ner = {}\n",
    "for headline in headlines[10:15]:\n",
    "    headline, response = ner_api_call(headline['title'])\n",
    "    headline_ner[headline] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f0e0e-1301-4bda-bd86-e12a964e06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in headline_ner.items():\n",
    "    print(k)\n",
    "    print(json.loads(v))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44114c60-b77a-4387-9578-e6da75b5357c",
   "metadata": {},
   "source": [
    "### Using LLMs for Sentiment Analysis\n",
    "We can use a similar approach to sentiment analysis on the headlines. Imagine we want ChatGPT to respond with a dictionary with positive, negative, and neutral values ranging from 0 to 1. You could manage this with a response format, but that might be overkill given the simplicity of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355fde3-630c-4641-9b9c-f70a921247d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sa_api_call(user_message):\n",
    "    system_message = \"\"\"\n",
    "    You are an expert in sentiment analysis. For each user input, analyze the sentiment and return a JSON dictionary with three keys: 'positive', 'negative', and 'neutral'. Each value should be a floating-point number between 0 and 1, representing the probability of each sentiment.\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        max_completion_tokens = 100,\n",
    "        temperature=.2,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_message,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    return user_message, json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adaa51b-aff2-4d58-9c61-51bf2487f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_api_call(headlines[10]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f0be5-1a5b-475f-97b3-b60968b35cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_api_call(headlines[11]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3eb8ba-4154-4482-9ea2-fc1abc4d75fe",
   "metadata": {},
   "source": [
    "## spacy-llm wrapper\n",
    "It's also possible to work with the ChatGPT API via different tools that have integrated LLMs into their own pipelines. spaCy, for example, offers a [spacy-llm package](https://spacy.io/usage/large-language-models) which provides pre-defined NLP tasks such as classifiers, NER, and summarization. \n",
    "\n",
    "Let's import spacy and load the `en_core_web_md` model that we worked with in previous lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d2f94-ddb9-43eb-9c79-1a1ba0147207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f809e-e213-4bc6-b5a7-4efa464cc545",
   "metadata": {},
   "source": [
    "spaCy comes with built-in pipelines. We can take a look at them here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af42fc8-cc91-40d7-af23-9386a55bd150",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca0263a-3420-4b3a-8a64-c0cada2c47df",
   "metadata": {},
   "source": [
    "This shows us the sequence of the pipes.\n",
    "\n",
    "1. 'tok2vec': This is the first step in the pipeline. It stands for \"tokenization to vectors\". This component converts text into numerical vectors that represent the semantic meaning of each token. It's a crucial preprocessing step for many other components.\n",
    "2. 'tagger': This component performs part-of-speech (POS) tagging. It assigns grammatical categories (like noun, verb, adjective, etc.) to each token in the text.\n",
    "3. 'parser': The parser analyzes the grammatical structure of the sentence. It determines the relationships between words and creates a dependency parse tree.\n",
    "4. 'attribute_ruler': This component can be used to add, modify or remove token attributes based on token or span matches. It's often used for rule-based corrections or additions to the pipeline's output.\n",
    "5. 'lemmatizer': The lemmatizer reduces words to their base or dictionary form. For example, \"running\" would be lemmatized to \"run\".\n",
    "6. 'ner': This stands for Named Entity Recognition. It identifies and classifies named entities (like persons, organizations, locations, etc.) in the text.\n",
    "\n",
    "This sequence represents a common order of operations in NLP:\n",
    "\n",
    "First, the text is tokenized and converted to vectors. Then, grammatical information is added (tagging and parsing). Additional attributes might be adjusted. Words are reduced to their base forms. Finally, named entities are identified.\n",
    "\n",
    "Each step in this pipeline builds on the previous ones, creating a rich set of linguistic annotations for the input text. This particular pipeline is quite comprehensive and would be suitable for a wide range of NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ddc2ea-7afb-47fa-8ba1-a655d1173e0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for n, headline in enumerate(headlines[100:150]):\n",
    "    print(n, headline['title'])\n",
    "    doc = nlp(headline['title'])\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "    print('---------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236856ec-a000-4c66-9d8b-9611754b7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = headlines[104]['title']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35169158-a2a1-44e4-a935-5bc49bdafc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503aa4c8-69b4-43bf-9c72-b884471ad143",
   "metadata": {},
   "source": [
    "The spaCy model we loaded analyzed this headline using the pipelines above, which we can access at the token level for each element in doc. Here are some of the different token attributes we can work with.\n",
    "\n",
    "| Name         | Description                                     | Code Example       |\n",
    "| ------------ | ----------------------------------------------- | ------------------ |\n",
    "| `sent`       | The sentence to which the token belongs.        | `token.sent`       |\n",
    "| `text`       | The raw text of the token.                      | `token.text`       |\n",
    "| `head`       | The parent of the token in the dependency tree. | `token.head`       |\n",
    "| `left_edge`  | The leftmost token of the token's subtree.      | `token.left_edge`  |\n",
    "| `right_edge` | The rightmost token of the token's subtree.     | `token.right_edge` |\n",
    "| `ent_type_`  | The entity type label of the token, if any.     | `token.ent_type_`  |\n",
    "| `lemma_`     | The lemmatized form of the token.               | `token.lemma_`     |\n",
    "| `morph`      | The morphological details of the token.         | `token.morph`      |\n",
    "| `pos_`       | The part of speech tag of the token.            | `token.pos_`       |\n",
    "| `dep_`       | The syntactic dependency relation.              | `token.dep_`       |\n",
    "| `lang_`      | The language of the parent document.            | `token.lang_`      |\n",
    "\n",
    "Let's look at some of the attributes of the 5th token, `Tagovailoa`, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a7809-a748-479d-99cb-6f253aba9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[4], doc[4].pos_, doc[4].ent_type_, doc[4].head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba8640-5d47-448e-af25-553b1c5ac978",
   "metadata": {},
   "source": [
    "You can guess from the `ent_type`, that it's possible to use spaCy's built-in NER, without resorting to an LLM. We can do that using the `.ents` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b370361-156c-4fd1-97f2-7844ebe14005",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5c9f7-51bd-4c04-b7ea-18febbc4e84d",
   "metadata": {},
   "source": [
    "But we can also build our own pipelines that integrate LLMs to help us process our texts, without relying on spaCy's \"en_core_web_md\" model. To do that we'll create our own configuration file and then \"assemble\" it. Let's import the `assemble` tool from the `spacy_llm` utility library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99775a4b-9d5f-4375-91fa-e5ee6fa96e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_llm.util import assemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10451d-d4f7-4cdf-a764-cc36c3e47f28",
   "metadata": {},
   "source": [
    "Now let's reload an nlp model from the config file in the `assets/` directory. We can take a look at the model design by selecting it from the file navigator to the left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c869e-4e87-42db-8634-73e23aa937ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = assemble(\"assets/openai-ner.cfg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e6d330-1e34-484e-b00e-8f5aeaaa2544",
   "metadata": {},
   "source": [
    "If we apply this model to the same headline we'll see it hasn't gone through the full \"en_core_web_md\" pipeline. Attributes like `.pos_` and `.head` aren't available because our config file doesn't create them. But we are able to build in more specific NER labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a0cef-320f-43ce-883d-88a8dbb066a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[4], doc[4].pos_, doc[4].ent_type_, doc[4].head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221954a-fc4b-43dd-9088-f008c7586ae1",
   "metadata": {},
   "source": [
    "But we do see at least one more entity identifed by the custom pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81f0a9-a63f-4b96-ac11-068b38c8558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fd439-4955-4406-b74a-8b18f0bc82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for headline in headlines[500:510]:\n",
    "    print('--------')\n",
    "    doc = nlp(headline['title'])\n",
    "    print(f'{doc.text} \\n')\n",
    "    for ent in doc.ents:\n",
    "        print(f'-- {ent.label_} {ent.text}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd84075-bd4d-4b5b-aa62-7325f9c865ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = assemble(\"assets/openai-textcat.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0586f3-fe7a-456b-a000-a1725aeecc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(headlines[0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e133f9b-a22c-4ef7-922a-d32ff3ba1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94570e-191a-4be3-bf17-4405c2e0fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c616396-30ac-4fea-8710-acbea0a97157",
   "metadata": {},
   "outputs": [],
   "source": [
    "for headline in headlines[500:510]:\n",
    "    print('--------')\n",
    "    doc = nlp(headline['title'])\n",
    "    print(f'{doc.text} \\n')\n",
    "    for ent in doc.ents:\n",
    "        print(f'-- {ent.label_} {ent.text}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5f463-a6bd-42ee-afa2-e9dfa327ef6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
